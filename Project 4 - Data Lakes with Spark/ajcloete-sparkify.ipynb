{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from datetime import datetime\nimport os\nfrom pyspark.sql import SparkSession, functions as F, Window, types as T", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fa1ed126d5844ec08aa6912b78157192"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fa1ed126d5844ec08aa6912b78157192"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\nStarting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1561470202522_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-32-70.us-west-2.compute.internal:20888/proxy/application_1561470202522_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-47-218.us-west-2.compute.internal:8042/node/containerlogs/container_1561470202522_0002_01_000002/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1561470202522_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-32-70.us-west-2.compute.internal:20888/proxy/application_1561470202522_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-47-218.us-west-2.compute.internal:8042/node/containerlogs/container_1561470202522_0002_01_000002/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\nSparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def create_spark_session():\n    \"\"\"\n    Create and return a spark session\n    \"\"\"\n    spark = SparkSession \\\n            .builder \\\n            .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n            .getOrCreate()\n    return spark\n\n\ndef process_song_data(spark, input_data, output_data):\n    \"\"\"\n    Process the song_data file using spark\n    spark(spark session object): The spark session to use\n    input_data(string): the location of the input data\n    output_data(string): the desired location of the output data\n    \"\"\"\n    # get filepath to song data file\n    song_data = input_data+'song_data/*/*/*/*.json'\n\n    # read song data file\n    df = spark.read.json(song_data)\n\n    # extract columns to create songs table\n    songs_table = df.select(['song_id','title','artist_id','year','duration']).distinct()\n\n    # write songs table to parquet files partitioned by year and artist\n    songs_table.write.parquet(output_data+'songs/'+'songs.parquet', partitionBy=['year','artist_id'])\n\n    # extract columns to create artists table\n    artists_table = df.select(['artist_id','artist_name','artist_location','artist_latitude','artist_longitude']) \\\n                    .withColumnRenamed('artist_name','artist') \\\n                    .withColumnRenamed('artist_location','location') \\\n                    .withColumnRenamed('artist_longitude','longitude') \\\n                    .withColumnRenamed('artist_latitude','latitude').distinct()\n\n    # write artists table to parquet files\n    artists_table.write.parquet(output_data+'artists/'+'artists.parquet')\n\n\ndef process_log_data(spark, input_data, output_data):\n    \"\"\"\n    Process the log_data file using spark\n    spark(spark session object): The spark session to use\n    input_data(string): the location of the input data\n    output_data(string): the desired location of the output data\n    \"\"\"\n    # get filepath to log data file\n    log_data = input_data+'log-data/*.json'\n\n    # read log data file\n    df = spark.read.json(log_data)\n\n    # filter by actions for song plays\n    df = df.where('page=\"NextSong\"')\n\n    # extract columns for users table    \n    w_user_ts = Window.partitionBy('userId').orderBy(F.desc('ts'))\n    users_table = df.withColumn('rn',F.row_number().over(w_user_ts)) \\\n                    .filter('rn=1') \\\n                    .withColumnRenamed('userId','user_id') \\\n                    .withColumnRenamed('firstName','first_name') \\\n                    .withColumnRenamed('lastName','last_name') \\\n                    .select(['user_id','first_name','last_name','gender','level'])\n\n    # write users table to parquet files\n    users_table.write.parquet(output_data+'users/'+'users.parquet')\n\n    # create timestamp column from original timestamp column\n    df = df.withColumn('timestamp', (F.col('ts')/1000).cast(dataType=T.TimestampType()))\n\n    # extract columns to create time table\n    time_cols = ['hour','day','week','month','year']\n    exprs = ['timestamp as start_time']+\\\n            ['extract({col} from timestamp) as {col}'.format(col=col) for col in time_cols]\n    weekDay = F.udf(lambda x: x.strftime('%w'))\n    time_table = df.selectExpr(*exprs).withColumn('weekday',weekDay('start_time')).distinct()\n\n    # write time table to parquet files partitioned by year and month\n    time_table.write.parquet(output_data+'time/'+'time.parquet', partitionBy=['year','month'])\n\n    # read in song data to use for songplays table\n    song_df = spark.read.json(input_data+'song_data/*/*/*/*.json')\n\n    # extract columns from joined song and log datasets to create songplays table \n    joined = df.join(song_df,(df.artist==song_df.artist_name)&(df.song==song_df.title),\n                     how='inner')\n    songplay_selects = ['songplay_id',\n                        'timestamp as start_time',\n                        'extract(month from timestamp) as month',\n                        'extract(year from timestamp) as year',\n                        'userId as user_id',\n                        'level','song_id','artist_id',\n                        'sessionId as session_id',\n                        'location',\n                        'userAgent as user_agent']\n    songplays_table = joined.distinct().withColumn('songplay_id', F.monotonically_increasing_id()) \\\n                        .selectExpr(*songplay_selects)\n\n    # write songplays table to parquet files partitioned by year and month\n    songplays_table.write.parquet(output_data+'songplays/'+'songplays.parquet', partitionBy=['year','month'])\n\n\ndef main():\n    \"\"\"\n    Run the ETL to process the song_data and the log_data files\n    \"\"\"\n    try:\n        _credentials(clean=True)\n    except Exception as e:\n        pass\n    spark = create_spark_session()\n    input_data = \"s3a://udacity-dend/\"\n    output_data = \"s3a://ajcloete-dend/sparkify/\"\n    \n    process_song_data(spark, input_data, output_data)    \n    process_log_data(spark, input_data, output_data)", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e9a39c7ed0794c2cbdbe3f3cfee77b44"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e9a39c7ed0794c2cbdbe3f3cfee77b44"}}, "metadata": {}}]}, {"metadata": {"scrolled": false, "trusted": true}, "cell_type": "code", "source": "main()", "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "46fe5861e5054e1eb5f2c2f5cb5a2cce"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "46fe5861e5054e1eb5f2c2f5cb5a2cce"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## The spark steps have been tested and works without a problem."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}